{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique characters that occur in this text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation - converting text (string) into sequences of integers\n",
    "\n",
    "Very simple tokenisation of charaters. If you want to you can use some more advanced tokenisation algorithms like 'tiktoken' from OpenAI using BPE tokeniser.\n",
    "\n",
    "**Example of tiktoken use:**\n",
    "\n",
    "```python\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\") #using encoding that was used during gpt2 training\n",
    "asert enc.decode(enc.encode(\"Hello world!\")) == \"Hello world!\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43, 2]\n",
      "General Kenobi!\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "#Encoder - takes a string (word for example) and output list of integers\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "\n",
    "#Decoder - takes a list of integers and output a string\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "#Examples of encoding and decoding\n",
    "print(encode('Hello there!'))\n",
    "print(decode(encode('General Kenobi!')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the entire text dtaset and store it into a torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\piotr\\AppData\\Local\\Temp\\ipykernel_7580\\841344146.py:3: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  data = torch.tensor(encode(text), dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the entire text in our dataset is 1:1 converted and represented by integers.\n",
    "\n",
    "Here is what it looks like on example of first 1000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate dataset into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First 90% of our dataset will be training dataset. The rest of it will become our validation dataset.\n",
    "n = int(0.9 * len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of maximum length chunk (chunk of text dataset that we will train our model on - one chunk at the time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]) the target most likely is 47\n",
      "When input is tensor([18, 47]) the target most likely is 56\n",
      "When input is tensor([18, 47, 56]) the target most likely is 57\n",
      "When input is tensor([18, 47, 56, 57]) the target most likely is 58\n",
      "When input is tensor([18, 47, 56, 57, 58]) the target most likely is 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]) the target most likely is 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target most likely is 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target most likely is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t + 1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context} the target most likely is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "Targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "-----------------\n",
      "When input is [24] the targets are 43\n",
      "When input is [24, 43] the targets are 58\n",
      "When input is [24, 43, 58] the targets are 5\n",
      "When input is [24, 43, 58, 5] the targets are 57\n",
      "When input is [24, 43, 58, 5, 57] the targets are 1\n",
      "When input is [24, 43, 58, 5, 57, 1] the targets are 46\n",
      "When input is [24, 43, 58, 5, 57, 1, 46] the targets are 43\n",
      "When input is [24, 43, 58, 5, 57, 1, 46, 43] the targets are 39\n",
      "When input is [44] the targets are 53\n",
      "When input is [44, 53] the targets are 56\n",
      "When input is [44, 53, 56] the targets are 1\n",
      "When input is [44, 53, 56, 1] the targets are 58\n",
      "When input is [44, 53, 56, 1, 58] the targets are 46\n",
      "When input is [44, 53, 56, 1, 58, 46] the targets are 39\n",
      "When input is [44, 53, 56, 1, 58, 46, 39] the targets are 58\n",
      "When input is [44, 53, 56, 1, 58, 46, 39, 58] the targets are 1\n",
      "When input is [52] the targets are 58\n",
      "When input is [52, 58] the targets are 1\n",
      "When input is [52, 58, 1] the targets are 58\n",
      "When input is [52, 58, 1, 58] the targets are 46\n",
      "When input is [52, 58, 1, 58, 46] the targets are 39\n",
      "When input is [52, 58, 1, 58, 46, 39] the targets are 58\n",
      "When input is [52, 58, 1, 58, 46, 39, 58] the targets are 1\n",
      "When input is [52, 58, 1, 58, 46, 39, 58, 1] the targets are 46\n",
      "When input is [25] the targets are 17\n",
      "When input is [25, 17] the targets are 27\n",
      "When input is [25, 17, 27] the targets are 10\n",
      "When input is [25, 17, 27, 10] the targets are 0\n",
      "When input is [25, 17, 27, 10, 0] the targets are 21\n",
      "When input is [25, 17, 27, 10, 0, 21] the targets are 1\n",
      "When input is [25, 17, 27, 10, 0, 21, 1] the targets are 54\n",
      "When input is [25, 17, 27, 10, 0, 21, 1, 54] the targets are 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 #How many independent sequences will be proccess in parallel\n",
    "block_size = 8 #What is the maximum context length for predictions\n",
    "\n",
    "#Function generating a samll batch of data of inputs x and targets y\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i + block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(\"Inputs:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"Targets:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"-----------------\")\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"When input is {context.tolist()} the targets are {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "#Input to the transformer\n",
    "print(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIGRAM LANGUAGE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 65])\n",
      "tensor(4.7673, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n",
      "\n",
      "Generated output is garbage because we have random untrained model.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) #Each token directly reads off the logits for the next token from a lookup table\n",
    "\n",
    "    def forward(self, idx, targets=None): #make targets oprtional cause of generating function\n",
    "        #idx and targets are both (B, T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) #(B, T, C) - B(batch_size), T(time - block_size), C(channel - vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            #We need to reshape our logits because it is (B, T, C) but torch cross entropy loss function expects C to be the 2nd parameter - [(B, C, T)]\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) #We are changing our tensor to 2-dimenasional tensor where B and T are stretched out to one dimension and in this way C is the 2nd dimension just as loss function expects\n",
    "            #We have to do the same thing to targets\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            #loss function\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    #Generation function takes idx (one block/sequenc) that is (B, T) and generate and concat it to be (B, T + max_new_tokens)\n",
    "    #For exmaple we give it idx with 8 characters (8 time steps) and want to generate 3 more time steps\n",
    "    #It will generate 3 new characters based on probability and distribution and our new idx will be (B, T+3)\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            #Get the predictions\n",
    "            logits, loss = self(idx)\n",
    "\n",
    "            #Focus only on the last time step\n",
    "            logits = logits[:, -1, :] #Become (B, C)\n",
    "\n",
    "            #Apply softmax to get probabilitics\n",
    "            probs = F.softmax(logits, dim = -1) #(B, C)\n",
    "\n",
    "            #Sample from distribiution\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) #(B, 1)\n",
    "\n",
    "            #Append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "#Generating\n",
    "idx_descripition = torch.zeros((1, 1), dtype = torch.long) #1:1 array holding 0 inside - thats how we kick off the generation (0 is an integer standing for the line break - new line character)\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype = torch.long), max_new_tokens=100)[0].tolist())) #0 unplugs existing batch dimensions to generate simple one-dimensional array of 100 timesteps which we will convert to simple python list and then decode\n",
    "\n",
    "print(\"\\nGenerated output is garbage because we have random untrained model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now our model does not use history. We feed him with entire sectence but he looks only at last pieces (last character). It is silly, but we want this function to stay. We want this function to be this way because eventually the history will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training BIGRAM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3) #We can use other optimisers (like SGB for example) but this is the most advanced and popular optimizer that works extremly well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4832067489624023\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for setps in range(20000):\n",
    "    #Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    #Evaluate loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fithods misue, knild he I:\n",
      "Whe! toudirer' My ayosbly louroura s m', uthos s reveprthoukerdi't avorure fotemowe.\n",
      "Whamo es t, tstt g t RTRushy,\n",
      "WAsbr spr my ou pl y,\n",
      "Witoft at o s me,\n",
      "Whabr'the Cicuomants awonte qungur thme wrar d parsupl by:\n",
      "'sul ve ave,\n",
      "Kconit ped bim; fam elathelch easutlll teye A \n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype = torch.long), max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Progress\n",
    "\n",
    "As we can see our model made some progress. It is not ideal but we can't expect it from BIGRAM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.rand(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want x[b, t] = mean_{i <= t} x[b, i]\n",
    "#To use history we just take average of previous tokens and current token (very weak and lossy method but we will optimize it later)\n",
    "xbow =torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev =x[b, :t+1]\n",
    "        xbow[b, t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0783, 0.4956],\n",
       "        [0.6231, 0.4224],\n",
       "        [0.2004, 0.0287],\n",
       "        [0.5851, 0.6967],\n",
       "        [0.1761, 0.2595],\n",
       "        [0.7086, 0.5809],\n",
       "        [0.0574, 0.7669],\n",
       "        [0.8778, 0.2434]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0783, 0.4956],\n",
       "        [0.3507, 0.4590],\n",
       "        [0.3006, 0.3156],\n",
       "        [0.3717, 0.4108],\n",
       "        [0.3326, 0.3806],\n",
       "        [0.3953, 0.4140],\n",
       "        [0.3470, 0.4644],\n",
       "        [0.4134, 0.4368]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "#EXAMPLE 1:\n",
    "\n",
    "#Toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)\n",
    "\n",
    "#Thanks to lower tril matrix and tranforming its rows into avarages we can use history by simple matrix product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EXAMPLE 2:\n",
    "\n",
    "#Using matrix multiply decribed in previous cell for a weighted aggregation\n",
    "w = torch.tril(torch.ones(T, T))\n",
    "w = w / w.sum(1, keepdim = True)\n",
    "\n",
    "#w are (T, T) pytorch will see that those matrix are diffrent sizes so will create a batch dimension in weights -- \n",
    "xbow2 = w @ x #(B, T, T) @ (B, T, C) = (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EXAMPLE 3:\n",
    "\n",
    "#Using softmax function\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "w = torch.zeros((T,T))\n",
    "w = w.masked_fill(tril == 0, float('-inf'))\n",
    "w = F.softmax(w, dim=-1)\n",
    "xbow3 = w @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-attention\n",
    "\n",
    "We don't want it to be simple average. Each token could find diffrent tokens more or less intresting and we want it to be data dependent.\n",
    "\n",
    "For example we may look for some constants in the past and we want to now what those constants are and this information to flow to us. \n",
    "\n",
    "We still want to gather information from the past but we want it to be data dapendent informations. That's the problem that self-attention solves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The wat self-attention solves it\n",
    "\n",
    "Every single node or token at each position will emit **2 vectors**:\n",
    "- **query** - we can discribe it as \"What I'm looking for?\"\n",
    "- **key** - we can discribe it as \"What do I contain?\"\n",
    "\n",
    "Having these 2 vector we simply do the **dot product** between the keys and the queries. So **my query** dot products with **all the keys of all the other tokens**.\n",
    "\n",
    "That dot product now becomes **weights**.\n",
    "\n",
    "If the key and the query are sort of aligned they will interact to a very high amount and them I will get to learn more about that specific token ass opposed to any other tokens in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.rand(B, T, C)\n",
    "\n",
    "#Implement a single one HEAD of self-attention\n",
    "#Every single independent token will produce key and query without eny communication\n",
    "head_size = 16 #hyperparameter\n",
    "key = nn.Linear(C, head_size, bias = False)\n",
    "query = nn.Linear(C, head_size, bias = False)\n",
    "value = nn.Linear(C, head_size, bias = False)\n",
    "\n",
    "k = key(x) #(B, T, 16[head_size])\n",
    "q = query(x) #(B, T, 16[head_size])\n",
    "\n",
    "#The communication starts now, every sigle query will dot product with every single key\n",
    "w = q @ k.transpose(-2, -1) #transposing last 2 dimensions ---------- now will be (B, T, 16) @ (B, 16, T) = (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "w = w.masked_fill(tril == 0, float('-inf')) #AD.1\n",
    "w = F.softmax(w, dim = -1)\n",
    "\n",
    "v = value(x)\n",
    "out = w @ v\n",
    "#out = w @ x\n",
    "\n",
    "out.shape\n",
    "\n",
    "#Now weights (weighted aggregation) will be function in data dependent manner between the keys and queries of these nodes\n",
    "\n",
    "'''\n",
    "Ad. 1. If we want ALL the nodes constantly talking to each other we just delete this line of code allowing them to do so.\n",
    "       In this case it is called 'encoder block'. It is usefull for example when we try to predict a sentiment of given text.\n",
    "       All nodes can talk to each other which makes prediction of sentiment better and possible.\n",
    "\n",
    "       In 'decoder block' this line is ALWAYS present. We make sure that we mask nodes with triangular matrix disallowing nodes\n",
    "       to talko to nodes from the future.\n",
    "\n",
    "       Both of those cases are allowed because attention doesn't care about it.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4409, 0.5591, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2975, 0.3373, 0.3652, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2211, 0.2898, 0.2236, 0.2654, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1832, 0.2163, 0.1954, 0.2437, 0.1614, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1330, 0.2227, 0.1784, 0.2159, 0.1044, 0.1456, 0.0000, 0.0000],\n",
       "        [0.1283, 0.1367, 0.1385, 0.1522, 0.1083, 0.1341, 0.2021, 0.0000],\n",
       "        [0.1064, 0.1332, 0.1265, 0.1445, 0.0940, 0.1200, 0.1231, 0.1524]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now every batch have diffrent sort of weights not the average weights beacuse every diffrent batch have diffrent tokens on diffrent positions\n",
    "#So now out weights are data dependent\n",
    "w[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explenation\n",
    "\n",
    "**[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],** \\\n",
    " **[0.4409, 0.5591, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],** \\\n",
    " **[0.2975, 0.3373, 0.3652, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],** \\\n",
    " **[0.2211, 0.2898, 0.2236, 0.2654, 0.0000, 0.0000, 0.0000, 0.0000],** \\\n",
    " **[0.1832, 0.2163, 0.1954, 0.2437, 0.1614, 0.0000, 0.0000, 0.0000],** \\\n",
    " **[0.1330, 0.2227, 0.1784, 0.2159, 0.1044, 0.1456, 0.0000, 0.0000],** \\\n",
    " **[0.1283, 0.1367, 0.1385, 0.1522, 0.1083, 0.1341, 0.2021, 0.0000],** \\\n",
    " **[0.1064, 0.1332, 0.1265, 0.1445, 0.0940, 0.1200, 0.1231, 0.1524]],**\n",
    " \n",
    "\n",
    "We can look at the **last row** for an example.\n",
    "\n",
    "Last row is for **8th** token. This token nows what content it has and knows at what position it's in.\n",
    "\n",
    "Based on that he creates a **query** like *'Hey I'm looking for this kind of staff. I'm a vowel and I'm on an 8th posisiton. I'm looking for any constants at positions up to 4'*.\n",
    "\n",
    "All the other nodes creates a **keys** and maybe one of the channels could be like *I am a constant and I am in a position up to 4*. That key would have a high number in that specific channel.\n",
    "\n",
    "That's how the **query** and the **key** hen they dot product, they can find each other and create a high affinity. When they have a high affinity like token **4** in **8th** row it means token 4 is interesting for token 8. In this situation through the softmax I will aggregate **a lot** of it's information into my position and so I'll get to learn a lot about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diference between self-attention and corss-attention\n",
    "\n",
    "In **self-attention** all **keys**, **queries** and **values** are comming from the same source (from x). We say that nodes are self-attending.\n",
    "\n",
    "In encoder-decoder transformer we can have a case where the **queries** are produced from x but the keys and values come from the whole separate **external source** (sometimes from encoder blocks thet encodes some context that we'd like to condition on). So in this case queries are produced by our internal nodes but the rest is produced from external nodes put aside. We are producing queries and we are reading information from the side. That attention is called **cross-attention**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lastly we have to add something to our weights.\n",
    "#We have to divide it by square root of head size. We have to do this because we don't want our values after softmax be to extreme.\n",
    "#Softmax would become way to peaky and will shapren to the max value in the nodes. So basically every node would gather information from just one simple node.\n",
    "\n",
    "#... (previous code)\n",
    "w = q @ k.transpose(-2, -1) * head_size**(-0.5)\n",
    "#...(rest of the code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same thing we can get by just calling nn.LayerNorm() so we don't need to implement that to main code\n",
    "class BatchNorm:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        xmean = x.mean(1, keepdim = True) #batch mean\n",
    "        xvar = x.var(1, keepdim = True) #batch variance\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) #normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
